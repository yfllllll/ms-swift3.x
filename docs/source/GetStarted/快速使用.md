# 介绍

Swift是一个提供LLM模型轻量级训练和推理的开源框架。Swift提供的主要能力是`efficient tuners`，tuners是运行时动态加载到模型上的额外结构，在训练时将原模型的参数冻结，只训练tuner部分，这样可以达到快速训练、降低显存使用的目的。比如，最常用的tuner是LoRA。

总之，在这个框架中提供了以下特性：

- **具备SOTA特性的Efficient Tuners**：用于结合大模型实现轻量级（在商业级显卡上，如RTX3080、RTX3090、RTX4090等）训练和推理，并取得较好效果
- **使用ModelScope Hub的Trainer**：基于`transformers trainer`提供，支持LLM模型的训练，并支持将训练后的模型上传到[ModelScope Hub](https://www.modelscope.cn/models)中
- **可运行的模型Examples**：针对热门大模型提供的训练脚本和推理脚本，并针对热门开源数据集提供了预处理逻辑，可直接运行使用

SWIFT库的github地址是：https://github.com/modelscope/swift

# 快速开始

在本章节会介绍如何快速安装swift并设定好运行环境，并跑通一个用例。

安装swift的方式非常简单，用户只需要在python>=3.8环境中运行：

```shell
# 全量能力
pip install ms-swift[all] -U
# 仅使用LLM
pip install ms-swift[llm] -U
# 仅使用AIGC
pip install ms-swift[aigc] -U
# 仅使用adapters
pip install ms-swift -U
```

SWIFT库提供了**LLM&AIGC模型的训练推理脚手架**，支持LLaMA、QWen、ChatGLM、Stable Diffusion等多种模型的直接训练和推理，并且集成了SWIFT库提供的tuners，
开发者可以直接使用。它们的位置在：https://github.com/modelscope/swift/tree/main/examples/pytorch/llm

- LLM训练和推理可以查看: [LLM微调文档](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLM微调文档.md)
- AIGC训练和推理可以查看: [文生图微调文档](https://github.com/modelscope/swift/blob/main/docs/source/AIGC/AnimateDiff微调推理文档.md)

# 使用WEB-UI训练和推理

SWIFT支持界面化训练和推理，只需要在执行上述的安装后启动web-ui即可：

```shell
swift web-ui
```

# 使用Tuners跑通一个示例

如果需要在自定义的训练流程中使用tuners，可以参考下面的代码。下面的代码使用LoRA在分类任务上训练了`bert-base-uncased`模型：

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from modelscope import AutoModelForSequenceClassification, AutoTokenizer, MsDataset
from transformers import default_data_collator

from swift import Trainer, LoRAConfig, Swift, TrainingArguments


model = AutoModelForSequenceClassification.from_pretrained(
            'AI-ModelScope/bert-base-uncased', revision='v1.0.0')
tokenizer = AutoTokenizer.from_pretrained(
    'AI-ModelScope/bert-base-uncased', revision='v1.0.0')
lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
model = Swift.prepare_model(model, config=lora_config)

train_dataset = MsDataset.load('clue', subset_name='afqmc', split='train').to_hf_dataset().select(range(100))
val_dataset = MsDataset.load('clue', subset_name='afqmc', split='validation').to_hf_dataset().select(range(100))


def tokenize_function(examples):
    return tokenizer(examples["sentence1"], examples["sentence2"],
    padding="max_length", truncation=True, max_length=128)


train_dataset = train_dataset.map(tokenize_function)
val_dataset = val_dataset.map(tokenize_function)

arguments = TrainingArguments(
    output_dir='./outputs',
    per_device_train_batch_size=16,
)

trainer = Trainer(model, arguments, train_dataset=train_dataset,
                    eval_dataset=val_dataset,
                    data_collator=default_data_collator,)

trainer.train()
```

在上面的例子中，我们使用了`bert-base-uncased`作为基模型，将LoRA模块patch到了['query', 'key', 'value']三个Linear上，进行了一次训练。

训练结束后可以看到outputs文件夹，它的文件结构如下：

> outputs
>
> ​    |-- checkpoint-xx
>
> ​                    |-- configuration.json
>
> ​                    |-- default
>
> ​                              |-- adapter_config.json
>
> ​                              |-- adapter_model.bin
>
> ​                    |-- ...

可以使用该文件夹执行推理：

```python
from modelscope import AutoModelForSequenceClassification, AutoTokenizer
from swift import Trainer, LoRAConfig, Swift


model = AutoModelForSequenceClassification.from_pretrained(
            'AI-ModelScope/bert-base-uncased', revision='v1.0.0')
tokenizer = AutoTokenizer.from_pretrained(
    'AI-ModelScope/bert-base-uncased', revision='v1.0.0')
lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
model = Swift.from_pretrained(model, model_id='./outputs/checkpoint-21')

print(model(**tokenizer('this is a test', return_tensors='pt')))
```
